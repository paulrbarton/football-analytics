name: Upload Data to MotherDuck

on:
  # Trigger when data files are pushed
  push:
    paths:
      - 'data/committed/**/*.csv'
      - 'data/committed/**/*.parquet'
  
  # Manual trigger
  workflow_dispatch:
    inputs:
      file_pattern:
        description: 'File pattern to upload (e.g., "nottingham_forest_*" or "premier_league_*")'
        required: false
        default: '*'
        type: string

jobs:
  upload:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install duckdb python-dotenv pandas
      
      - name: Upload to MotherDuck
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
          DUCKDB_MOTHERDUCK_DISABLE_SSL_VERIFICATION: 1
        run: |
          python - <<'EOF'
          import os
          import duckdb
          from pathlib import Path
          import sys
          
          # Configuration
          token = os.getenv('MOTHERDUCK_TOKEN')
          database = 'football_analytics'
          schema = 'raw'
          file_pattern = '${{ github.event.inputs.file_pattern || '*' }}'
          
          if not token:
              print("âŒ MOTHERDUCK_TOKEN not set")
              sys.exit(1)
          
          # Connect to MotherDuck
          print(f"Connecting to MotherDuck database: {database}")
          conn = duckdb.connect(f'md:{database}?motherduck_token={token}')
          
          # Create schema if not exists
          conn.execute(f"CREATE SCHEMA IF NOT EXISTS {schema}")
          print(f"âœ“ Schema '{schema}' ready")
          
          # Find all data files
          data_dir = Path('data/committed')
          if not data_dir.exists():
              print("âš ï¸  No data/committed directory found")
              print("Run your scraper locally and commit files to data/committed/")
              sys.exit(0)
          
          # Process CSV files
          csv_files = list(data_dir.glob(f'**/{file_pattern}.csv'))
          parquet_files = list(data_dir.glob(f'**/{file_pattern}.parquet'))
          
          all_files = csv_files + parquet_files
          
          if not all_files:
              print(f"âš ï¸  No files matching pattern '{file_pattern}' found in data/committed/")
              sys.exit(0)
          
          print(f"\nFound {len(all_files)} file(s) to upload:")
          for file in all_files:
              print(f"  - {file}")
          
          # Upload each file
          success_count = 0
          for file_path in all_files:
              # Derive table name from filename (remove extension)
              table_name = file_path.stem
              full_table = f"{schema}.{table_name}"
              
              try:
                  print(f"\nðŸ“¤ Uploading {file_path.name} to {full_table}...")
                  
                  # Read file and upload
                  if file_path.suffix == '.csv':
                      conn.execute(f"""
                          CREATE OR REPLACE TABLE {full_table} AS 
                          SELECT * FROM read_csv_auto('{file_path}')
                      """)
                  else:  # parquet
                      conn.execute(f"""
                          CREATE OR REPLACE TABLE {full_table} AS 
                          SELECT * FROM read_parquet('{file_path}')
                      """)
                  
                  # Get row count
                  count = conn.execute(f"SELECT COUNT(*) FROM {full_table}").fetchone()[0]
                  print(f"   âœ“ Uploaded {count} rows to {full_table}")
                  success_count += 1
                  
              except Exception as e:
                  print(f"   âŒ Failed to upload {file_path.name}: {e}")
          
          conn.close()
          
          print(f"\n{'='*50}")
          print(f"âœ“ Upload complete: {success_count}/{len(all_files)} files successful")
          print(f"{'='*50}")
          
          if success_count < len(all_files):
              sys.exit(1)
          EOF
      
      - name: Summary
        if: always()
        run: |
          echo "## Upload Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- Database: football_analytics" >> $GITHUB_STEP_SUMMARY
          echo "- Schema: raw" >> $GITHUB_STEP_SUMMARY
          echo "- Files processed from: data/committed/" >> $GITHUB_STEP_SUMMARY
